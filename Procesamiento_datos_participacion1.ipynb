{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04c06eb-133f-45b0-bdf2-d8cc86c36228",
   "metadata": {},
   "source": [
    "Participacion 1.\n",
    "### Participación (2 puntos)\n",
    "\n",
    "Una de dos:\n",
    "- Aplicar la técnica de _stem_ o lematización para analizar la frecuencia de palabras de esta obra\n",
    "- Implementar una manera de obtener la frecuencia de $n$-gramas de cualquier obra antes de que se acabe la clase.\n",
    "- Subir la solución propuesta al repositorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffac2d-8b4d-4105-bbcf-ea42da081164",
   "metadata": {},
   "source": [
    "¿Que es la tecnica de lematizacion?\n",
    "- Permite reducir las palabras a sus raíces o formas base, lo que facilita el análisis de frecuencia al agrupar terminos similares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456312ec-7077-4292-bbbf-0e2207cec956",
   "metadata": {},
   "source": [
    "Que son los n-gramas? \n",
    "- Son secuencias de caracteres o elementos consecutivos que se extraen de un texto o discurso\n",
    "- Pueden ser: Letras Sílabas Palabras Signos de puntuación Espacios en blanco Fonemas Pares de bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df807b75-5a52-45dc-bddf-c6f207be1765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gutenbergpy in c:\\users\\user\\anaconda33\\lib\\site-packages (0.3.5)\n",
      "Requirement already satisfied: future>=0.15.2 in c:\\users\\user\\anaconda33\\lib\\site-packages (from gutenbergpy) (1.0.0)\n",
      "Requirement already satisfied: httpsproxy-urllib2 in c:\\users\\user\\anaconda33\\lib\\site-packages (from gutenbergpy) (1.0)\n",
      "Requirement already satisfied: lxml>=3.2.0 in c:\\users\\user\\anaconda33\\lib\\site-packages (from gutenbergpy) (5.2.1)\n",
      "Requirement already satisfied: pymongo in c:\\users\\user\\anaconda33\\lib\\site-packages (from gutenbergpy) (4.10.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\user\\anaconda33\\lib\\site-packages (from gutenbergpy) (69.5.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\user\\anaconda33\\lib\\site-packages (from gutenbergpy) (4.0.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\user\\anaconda33\\lib\\site-packages (from pymongo->gutenbergpy) (2.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\USER\\anaconda33\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\USER\\anaconda33\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\USER\\anaconda33\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gutenbergpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1d445c-b327-4578-a4fe-cc09c58253b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Librerias e imports\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from gutenbergpy.textget import get_text_by_id, strip_headers\n",
    "import pandas as pd\n",
    "\n",
    "# Stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Descargar el libro de Orgullo y Perjuicio id = 1342\n",
    "def obtener_libro(id=1342):\n",
    "    raw_book = get_text_by_id(id)  # Texto con encabezados\n",
    "    clean_book = strip_headers(raw_book)  # Texto limpio sin encabezados\n",
    "    return clean_book.decode('utf-8'), raw_book.decode('utf-8')  # Decodificar bytes a string para utilizar las librerias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22e456a-87f6-4dd1-8773-b61539e5a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar el libro\n",
    "clean_book, raw_book = obtener_libro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2282a691-1e12-4484-9afc-4fdf3df32655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar el texto - Tokenizar el texto\n",
    "tokens = word_tokenize(clean_book.lower())  # Convertir a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be18ce97-5230-4b62-83db-d9fe08863bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar palabras vacías y caracteres no alfabéticos\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d448ae6-e44b-4512-aecc-9026856f1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logica de negocio\n",
    "# Steming\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "# Lemmatización\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "# Frecuencia de palabras\n",
    "frecuencia_palabras_lenmas = Counter(lemmas)  # Usa stems o lemmas según prefieras\n",
    "frecuencia_palabras_stem = Counter(stems) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204005cb-447b-40ec-bdc0-bc964528479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la frecuencia de n-gramas utilizando las palabras tokenizadas\n",
    "# Como segundo parametro es sobre si queremos bigrama, trigramas, etc. El usado en clase fue el n = 2 (bigramas)\n",
    "def obtener_ngramas(tokens, n):\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])  # Genera n-gramas\n",
    "    ngram_freq = Counter(ngrams)\n",
    "    return ngram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96207c6a-bc6a-4292-998d-f777bc6c91e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Bigramas mas comunes:\n",
      "[(('lady', 'catherine'), 122), (('miss', 'bingley'), 87), (('miss', 'bennet'), 67), (('said', 'elizabeth'), 46), (('sir', 'william'), 44), (('de', 'bourgh'), 40), (('miss', 'darcy'), 39), (('young', 'man'), 37), (('colonel', 'fitzwilliam'), 30), (('dare', 'say'), 28)]\n"
     ]
    }
   ],
   "source": [
    "bigramas = obtener_ngramas(filtered_tokens, 2)\n",
    "print(\"10 Bigramas mas comunes:\")\n",
    "print(bigramas.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "660096e0-8976-4bfe-b510-474490fe6789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras mas comunes con lenmas:\n",
      "[('elizabeth', 643), ('could', 529), ('would', 482), ('darcy', 424), ('said', 406), ('bennet', 346), ('much', 332), ('miss', 315), ('must', 311), ('bingley', 307)]\n"
     ]
    }
   ],
   "source": [
    "# Palabras mas comunes\n",
    "print(\"Palabras mas comunes con lenmas:\")\n",
    "print(frecuencia_palabras_lenmas.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3518aa-0090-4be0-99f8-61a34e5f53de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras mas comunes con stems:\n",
      "[('elizabeth', 643), ('could', 529), ('would', 482), ('darci', 424), ('said', 406), ('bennet', 346), ('much', 332), ('miss', 319), ('bingley', 313), ('must', 311)]\n"
     ]
    }
   ],
   "source": [
    "# Palabras mas comunes con lenmas\n",
    "print(\"Palabras mas comunes con stems:\")\n",
    "print(frecuencia_palabras_stem.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8565682-7395-4221-9058-2aa20957ef56",
   "metadata": {},
   "source": [
    "La lematizacion no está alterando mucho las palabras porque ya están en una forma bastante estándar (como \"elizabeth\" y \"darcy\"), \n",
    "El stemmer esta modificando más algunas palabras, como \"darcy\" que se convierte en \"darci\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "101233cc-b121-451d-88b2-978db2cbaabd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_lenmas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_lenmas \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: words_lenmas, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconteo\u001b[39m\u001b[38;5;124m'\u001b[39m: top_lenmas})\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Gráfica Lematizadas\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words_lenmas' is not defined"
     ]
    }
   ],
   "source": [
    "df_lenmas = pd.DataFrame({'token': words_lenmas, 'conteo': top_lenmas})\n",
    "\n",
    "# Gráfica Lematizadas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_lenmas['token'], df_lenmas['conteo'], color='black')\n",
    "plt.xlabel('Palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Frecuencia de Palabras Lematizadas')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa8e188-1b47-4958-8ff6-d0a3b82182e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_stems' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df_stems \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: words_stems, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconteo\u001b[39m\u001b[38;5;124m'\u001b[39m: top_stems})\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Grafica de barras Steam\u001b[39;00m\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words_stems' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df_stems = pd.DataFrame({'token': words_stems, 'conteo': top_stems})\n",
    "\n",
    "# Grafica de barras Steam\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_stems['token'], df_stems['conteo'], color='orange')\n",
    "plt.xlabel('Palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Frecuencia de Palabras con Stems')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b0426-c0a1-4ce9-9d12-f66c1c3ca977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Barra apiladas\n",
    "df_lenmas = pd.DataFrame({'token': words_lenmas, 'conteo': top_lenmas})\n",
    "df_stems = pd.DataFrame({'token': words_stems, 'conteo': top_stems})\n",
    "\n",
    "# Unir los dos dataframe en uno solo\n",
    "df_combined = pd.DataFrame({\n",
    "    'token': words_lenmas + words_stems,\n",
    "    'conteo': top_lenmas + top_stems,\n",
    "    'tipo': ['Lematizadas'] * len(top_lenmas) + ['Stems'] * len(top_stems)\n",
    "})\n",
    "\n",
    "df_pivot = df_combined.pivot_table(index='token', columns='tipo', values='conteo', aggfunc='sum', fill_value=0)\n",
    "\n",
    "df_pivot.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "\n",
    "# Configuración del gráfico\n",
    "plt.xlabel('Palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Frecuencia de Palabras: Lematizadas vs Stems (Apilado)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1a404-35a2-4101-8a7d-288b2a7df47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Extraer los 10 bigramas mas comunes, convertirlo a data frame\n",
    "top_bigrams = bigramas.most_common(10)\n",
    "df_bigrams = pd.DataFrame(top_bigrams, columns=['bigram', 'frecuencia'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot de los bigramas más comunes\n",
    "plt.bar([' '.join(bigram) for bigram in df_bigrams['bigram']], df_bigrams['frecuencia'], color='violet')\n",
    "plt.xlabel('Bigramas')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('10 Bigramas Más Comunes')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
